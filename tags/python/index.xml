<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>python on 1ミリもわからん</title><link>https://raahii.github.io/tags/python/</link><description>Recent content in python on 1ミリもわからん</description><generator>Hugo -- gohugo.io</generator><language>ja-JP</language><lastBuildDate>Sun, 04 Aug 2019 04:13:59 +0900</lastBuildDate><atom:link href="https://raahii.github.io/tags/python/index.xml" rel="self" type="application/rss+xml"/><item><title>画像データをサーバーにPOSTする</title><link>https://raahii.github.io/2019/08/04/files-upload/</link><pubDate>Sun, 04 Aug 2019 04:13:59 +0900</pubDate><guid>https://raahii.github.io/2019/08/04/files-upload/</guid><description>機械学習を使ったサービス/アプリを開発しているとクライアントから画像をサーバーに送って推論して結果を返す，ということをよくやるのでメモ．
1枚しか送らない場合 今の所自分はこのパターンが多いです．いくつか実現方法はあると思いますが，リクエストボディに直接画像データのバイナリを入れて送る方法がシンプルで好きです．クライアント側のコードはこんな感じ．
import json import urllib.parse import urllib.request # read image data f = open(&amp;#34;example.jpg&amp;#34;, &amp;#34;rb&amp;#34;) reqbody = f.read() f.close() # create request with urllib url = &amp;#34;http://localhost:5000&amp;#34; req = urllib.request.Request( url, reqbody, method=&amp;#34;POST&amp;#34;, headers={&amp;#34;Content-Type&amp;#34;: &amp;#34;application/octet-stream&amp;#34;}, ) # send the request and print response with urllib.request.urlopen(req) as res: print(json.loads(res.read())) 注意点として Content-Type に application/octet-stream を指定すると良いです．このMIMEタイプは曖昧なバイナリデータを指しており，ファイル種別を特に指定しないことを意味します（ref: MIME type: application/octet-stream ）．
urllibの場合，これを指定しないとPOSTのデフォルトのMIMEタイプである application/x-www-form-urlencoded となり，サーバー側で正しく受け取れないので気をつけてください．
一方でサーバー側（flaskの場合）のコードはこのようになります．画像データをOpenCVで読んで画像のshapeをjsonで返しています．
@app.route(&amp;#34;/&amp;#34;, methods=[&amp;#34;POST&amp;#34;]) def example(): # read request body as byte array _bytes = np.</description></item><item><title>tensorboard-chainerにビデオを記録するためのPRを出した</title><link>https://raahii.github.io/2018/05/13/add-video-method-for-tensorboard-chainer/</link><pubDate>Sun, 13 May 2018 21:37:27 +0900</pubDate><guid>https://raahii.github.io/2018/05/13/add-video-method-for-tensorboard-chainer/</guid><description>機械学習における可視化ツールの1つにTensorBoardがある。これはTensorflowに付属しているソフトウェアで、学習時のlossやaccuracy、重みのヒストグラムなどを記録することができる。加えて、画像や音声などのデータも記録出来るので、生成モデルの学習でも便利に使える。
自分は普段Chainerで書いていてそのままではtensorboardは使えないのでtensorboard-chainerを使わせてもらっている。これとてもありがたい。
ただ、研究テーマが動画生成なので、動画も記録できれば便利なのに…とずっと思っていた。最近真面目にどうにか出来ないかと思って調べたら.gifの記録は元々できるらしいことがわかった。
Video summary support · Issue #39 · tensorflow/tensorboard · GitHub ということで、動画を記録できるメソッドを実装してプルリクエストを出した。初めて出したのだけれど、カバレッジやコード規約をチェックしてくれるツールに初めて触れた。外からだとテストが通らなかった理由がいまいちわからないので若干困ったけど、慣れれば便利そう。とりあえずマージはされたので良かったです。
add method &amp;ldquo;add_video&amp;rdquo; to SummaryWriter by raahii · Pull Request #2 · neka-nat/tensorboard-chainer · GitHub ということでtensorboard-chainerのadd_videoメソッドで動画記録できます。fpsも指定できます。便利。</description></item><item><title>3DGANをchainerで実装した</title><link>https://raahii.github.io/2017/10/25/chainer-implementation-3dgan/</link><pubDate>Wed, 25 Oct 2017 20:14:00 +0900</pubDate><guid>https://raahii.github.io/2017/10/25/chainer-implementation-3dgan/</guid><description>タイトルの通り，3DGANのchainer実装をgithubに上げた．当初はKerasで書いていたが良い結果が得られず，ソースコードの間違い探しをするモチベーションが下がってきたので，思い切ってchainerで書き直した．
実はmnistなどのサンプルレベルのものを超えてちゃんとディープラーニングのタスクに取り組むのは今回が初めてだった． ChainerによるGANの実装自体は公式のexampleやchainer-gan-libが非常に参考になった．
モデル 3DGANはその名の通り3Dモデルを生成するためのGAN（Voxelです）．Learning a Probabilistic Latent Space of Object Shapes via 3D Generative-Adversarial Modelingで提案されているもの．前回の記事でも触れた．
構造はDCGANと同様で200次元のベクトルよりGeneratorでサンプルを生成，Discriminatorでデータセット由来かGenerator由来か（real/fake）を分類しそのロスをフィードバックする．
Generatorは以下の図（論文より引用）のようなネットワークで，Discriminatorはこれを反転したようなモデルになっている．各ネットワーク内では3次元畳み込みを使用する．最適化手法はAdamで，論文ではDiscriminatorがバッチを8割以上正しく分類できた場合はパラメータを更新しないようにしたとあった．
3Dモデル データセットにはShapeNet-v2を用いた．このデータセットには様々な種類の3Dモデルが収録されているが，今回は椅子のモデルのみを抽出した．椅子はおよそ6700サンプルが収録されており，ファイル形式は.binvoxが直接収録されていたのでそれを使用した．
ただ，6700サンプルの3Dデータを全てメモリに乗せることはできなかったため，初期実装では毎回のループで読み込み処理を行っていた．その後，.binvoxファイルのヘッダー読み込みなどが不要であり，処理速度に支障があると感じたので事前に.h5に書き出して使うようにした．
ShapeNet-v2に収録されているデータのサンプルを示す．
実装 3DGANの実装をやろうと決めてから，実験を始める前に3Dモデルの取扱について理解するためのツールをつくっていた．主にはSimple Voxel Viewerで，.binvox形式について理解したり，matplotlibでボクセルをどうやってプロットしようかということについて考えていた．
64x64x64のボクセルを可視化するため，最初はmatplotlibの3Dplotを試したが，scatter plotやsurface plotを使うとマインクラフトのような箱を集積した見映えのプロットが実現できない上，一つ描画するのに数十秒かかることがわかった．そこからまず自作してみようと思いTHREE.jsを使ってSimple Voxel Viewerを作ってみた．ところが結局こっちもいくらか高速化は試したものの，64x64x64のサイズでも密なボクセルになるとメモリーエラーが起こってしまいうまく動作しない問題が起こった．加えて当たり前だがPythonのコードにも組み込めない．
そうして結局，matplotlibの3D volxe plotを採用した．しかしこの関数はまだリリースされていないため（2017/10時点），githubから直接インストールする必要があった．動作も遅いままだが妥協することにした．
ネットワークはKerasやTensorflowなどによる実装がいくつかgithubに上がっていたためそれらも参考にしつつ実装した．加えて，有名なGANのベストプラクティスのページを参考にした．ポイントをかいつまむと以下のような感じで実装した．
ランダムベクトルzは論文では一様分布だったがガウス分布を使った． GeneratorはDeconv3D+BN+ReLUの繰り返しで，最後だけsigmoid． DiscriminatorはConv3D+BN+Leaky-ReLUの繰り返しで，最後だけsigmoid． Chainerの公式のexampleを真似してロスはsoftplusを使って実装．ただ，実はsigmoid + Adversarial lossがsoftplusと同じなのでDiscriminatorの最後のsigmidは不要なのだが，加えた方がうまくいった(謎)． 結果 成功例 良さげな感じを出すためにきれいなものを集めた．学習の初期段階ではでたらめなものが出力されるが，徐々に椅子が形成され，50エポックから100エポックくらいでましなものが出来た．
学習の途中では椅子とは独立した無意味なかたまりのオブジェクトが所々に浮かんでいたりしたが，それが消えてくるとかなり見栄えが良くなっていった．
失敗例 ボクセルが全て1になったり0になって消滅したりした．今回幾度も学習をさせてみて，初期の段階からほぼ1なボクセル，あるいはほぼ0なボクセルが生成されたり，規則的なパターン（模様）を持つボクセルが生成されたりすると多くの場合失敗となるという微妙な知見を得た．
また，ボクセルが消滅したらその後復活しないこともわかった．ただこれは実装のところで述べたようにロスが間違っているせいかもしれない．
わかったこと GANはロスは全くあてにならない．生成結果が全て． zはガウス分布から取ってきたほうが良さそう． 学習を調整する(Discriminatorのlossやaccを見て更新しないなど）のはうまくいかないと感じた． 今回のコードではsigmoid + adversarial lossをsoftplusで実装しているので，Discriminatorの最後のsigmoidは不要なはずなのだが，誤って入れていたらうまくいき，外したらうまくいかなくなった．動きゃ勝ちみたいなところがあって釈然としない． 論文では1000エポック学習したとあったが100エポック行かないくらいでかなり形になった． また，今回はGANのベストプラクティスの内，以下のトリックは実践しても効果がなかった.
Discriminatorに学習させるミニバッチをrealのみまたはfakeのみにする．(項目4) GeneratorにもDiscriminatorにもLeaky-ReLUをつかう．(項目5) GeneratorにADAMを使ってDiscriminatorにはSGDを使う．(項目10) GeneratorにDropoutを使う．(項目17) 所感 実装に関して，やはりコード自体はKerasの方が圧倒的に簡単にかけるようになっているなと感じた．モデルのインスタンスを作ってバッチをmodel.</description></item><item><title>imgcatコマンドで遊ぶ</title><link>https://raahii.github.io/2016/09/21/show-lgtm-using-giphy-and-imgcat/</link><pubDate>Wed, 21 Sep 2016 23:10:00 +0900</pubDate><guid>https://raahii.github.io/2016/09/21/show-lgtm-using-giphy-and-imgcat/</guid><description>近況
インターンに行ってJavaを用いたWebアプリケーション開発を経験してきました。
もともとサーバーサイドの方の知識は0に近く、データベースとかサーバーってめんどくさそう…くらいの認識でした。今回その辺りのコーディングをいくつか担当させて頂き、Webアプリの全体像が見えた気がします。とりあえず、Webアプリ開発を一通り経験したというのはとても大きな意味がありました。
また、チーム開発が初めてだったこともあり、Gitを初めて実践的に使った他、かんばんやKPTといったアジャイル的な開発手法にも触れられたのも楽しかったです。
imgcatコマンド がらっと話は変わりますが本題。みなさんimgcatというコマンドをご存知でしょうか。おそらくiTerm上でしか動かない…と思いますが、ターミナル上で画像を表示するコマンドです。
これ、一見ネタのようなコマンドですが、Qiitaにはこんな記事が投稿されています。
qiita.com
いや、やっぱりネタかもしれない。
こんなimgcatですが、もしかしたらこれってすごい力を秘めているのではないかと私は思いました。というのも、黒い画面というのはどうしても地味になりがちで、長時間コーディングをすると精神的に良くないと感じるからです。これを使えばもしかしたらターミナルが賑やかになるかもしれない...！
GIPHYからGIF画像を取得して表示する ということで、いつまでも自分の手元にある画像を見ていても面白くないのでネットから拾ってきます。最初はGoogle画像検索を使おうと思っていましたが、最終的にGIPHYというサイトのAPIを使ってGIF画像を取ることにしました。そうです、imgcatでGIF画像を表示するとちゃんと動くんです！できたものはこんな感じ。
猫。
ピカチュウ。
カートマン。
GIPHYは海外サイトなので日本語では検索できませんが、結構素材は豊富っぽいです。
ソースコード Pythonで書きました。簡単ですが...。 コレくらいだったらwgetとかでワンライナーで書けたりしそう。どうだろう。
手順はこんな感じです。
コマンドライン引数で検索ワードを受け取る
GIPHYのAPIを使って画像を検索し、結果からランダムに一つをピックアップする
選んだ画像のURLにHTTPリクエストを投げてかえってきた画像データをそのままバイナリで標準出力に流す
imgcatにリダイレクトする
GIPHYのAPIはGoogleのCustom Search APIと違って（おそらく）制限がないのと、現在public beta keyを出してくれてるので使うのが楽でした。
また、今回使った検索以外にもトレンドの画像の取得や絵文字からGIFへの変換など色々できるようで今度使ってみたいなと思います。
応用例 さて、そもそもこれを作ったワケというのは、黒い画面を眺め続け疲弊した心に安らぎをあたえてやることでした。
一つ考えた例としてgit commitする度に好きなテーマの画像が表示されるようにします。zshrcに以下を追加。
すると…
予想外に地味😇。　今回は検索ワードを&#34;LGTM&#34;にしましたが、猫でいいかも。git commitをmycommitに置き換えなきゃいけないのはスマートじゃないですね。
ということで、みなさんもくれぐれも心のケアは大切にして下さい（適当）。
終わりに こういうの作ってる最中はいいんだけど、作り終わった後の賢者タイムの辛さ…ね…。
所感 拾ってくる画像
Twitterからとってきても面白いかも。
最後の使い方の例のところ改良の余地有り
元々git commitしたら画像を表示するというのは、たまたま見かけたcdしたらlsするという記事にヒントを得たものでした。なので本当はgit commit() { \gitcommit &#34;</description></item><item><title>外部モニターで動画を見ると辛い</title><link>https://raahii.github.io/2016/06/23/euro2016-result-prediction/</link><pubDate>Thu, 23 Jun 2016 17:28:00 +0900</pubDate><guid>https://raahii.github.io/2016/06/23/euro2016-result-prediction/</guid><description>こんにちは。EURO2016盛り上がってますね。みなさん見ていますか。明後日からはトーナメントが始まりますが僕の予想はコレです。
フランス優勝とイタリアが勝ち上がるとこがミソです。山が違っていたら決勝はイタリアvsフランスにしてました。まぁぼく欧州サッカー全然知りませんけど笑
という感じで、最近はEURO2016の試合ハイライトをよく見るのですが、動画鑑賞においてはMacbook Airが思ったより非力で辛いです。
特に、外部モニタで視聴すると、うなる。
ちなみに自分のMBAは2013年モデルの11インチで、cpuはi7の方なのですが、普通にMacのモニタで見ている分には特にcpu（ファン）は暴走しません。反対に、外部接続しているモニタは23インチで、これで見ているとcpuファンがかなり回り始めます。でかいモニタを使うとやはりレンダリングとかの関係で重いんですかね？
ということで、ちょっと気になったので簡単に可視化してみました。
方法はMacモニタと外部モニタでそれぞれ動画を最大化して視聴し、cpu使用率を計測します。ちなみにGoogle Chromeでニコニコ動画を見ました🍺。シンプル。
一応少し頑張ってスクリプトを…。cpu使用率を取得するのはshellscriptで、グラフ化はpythonでやりました。
んで結果、Macモニタの場合 外部モニタの場合
という感じでした。グラフの背景が白で汚い…。
とりあえず、外部モニタの場合，Macのモニタに比べて30%近くcpu使用率が高いという結果に。今回はMacを起動した後に、GoogleChromeだけを立ち上げて動画を視聴という流れで揃えたので、これでもcpu使用率は差が出てない方だと思います。普段からなんとなーくアクティビティモニタを開いて見たりしていますが、他のタブや他のアプリケーションを同時に開いていると、二倍近く差が出る時もあった気がします。やっぱりcpuが非力だと外部モニタって負荷でかいんですね…。
あと、MacはIntelのcpuを積んでいるので、いい感じにオーバークロックして処理性能を上げる「Turbo Boost」という機能がついているみたい。ただ、排熱効率に優れないMacの場合これが原因でcpu温度がみるみる上昇していきます。cpu負荷が大きいプロセスを実行すると、このおせっかい機能によって熱暴走がおきてcpuファンの回転に拍車をかける─ これも原因の一つかなぁ。まぁそんな感じです。
あと今回書いたスクリプトの方は、シェルスクリプトでpsコマンドの出力をcutできなくてちょっと躓きました。結局awkで解決したので、もっとawkを使いこなしたい。あと、シェルスクリプトはスペースが入る文字列を扱うときにわけわからなくなったりするので&#34;と&#39;の違いとかをちゃんと覚えないとダメかも。日頃からもっと頻繁に書いていきたいです。
Pythonの方は去年一年間使ったのでわりとスラスラ書けた。最近はC/C++ばっかり使っているのであれだけど、やっぱりメソッドチェーンは慣れないと読みづらい気がする。matplotlibはとても使いやすいので好き。
grep,sed,awk
作者: 美吉明浩 出版社/メーカー: 秀和システム 発売日: 1998/05/29 メディア: 単行本 この商品を含むブログを見る
コレ読みたい。それでは。</description></item><item><title>Pythonの日本語文字列</title><link>https://raahii.github.io/2015/06/28/nlp-100-knock/</link><pubDate>Sun, 28 Jun 2015 21:10:00 +0900</pubDate><guid>https://raahii.github.io/2015/06/28/nlp-100-knock/</guid><description>「研究者流コーディングの極意」を読んで、なんだかためになりそうだし、面白そうだし、ということで言語処理100本ノックを始めてみました。そして2つ目で詰まった(早い)。
使っている言語はPythonで、使い始めたばかりなのですが、そもそもプログラミングがダメダメです。
まず、その問題ですが、
&amp;lt;blockquote&amp;gt; &amp;lt;p&amp;gt;01. 「パタトクカシーー」&amp;lt;br&amp;gt; 「パタトクカシーー」という文字列の1,3,5,7文字目を取り出して連結した文字列を得よ．
&amp;lt;/blockquote&amp;gt; です。
まず私が考えたのは、こんな感じです。なんの疑問もなくこれでいけるだろうと思ってました笑。
string=&amp;ldquo;パタトクカシーー&amp;rdquo; rev=&amp;ldquo;&amp;rdquo; for i in [1,3,5,7]: rev+=string[i] print rev
一方結果は、
% python 01.「パタトクカシーー」.py �㿃 なんか文字化けしてる…。
それもそのはずで、「パタトクカシーー」は全て全角文字なのでひとひねり必要です。一般に半角は1byte、全角は2byteに符号化されているので、配列のお部屋と1:1対応にならないのが原因。
そしてPythonの場合、通常のstr型の全角文字は3byteに符号化されている（お部屋3つに対応している）みたいです。
&amp;gt;&amp;gt;&amp;gt; &amp;lsquo;あ&amp;rsquo;[0] &amp;lsquo;\xe3&amp;rsquo; &amp;gt;&amp;gt;&amp;gt; &amp;lsquo;あ&amp;rsquo;[1] &amp;lsquo;\x81&amp;rsquo; &amp;gt;&amp;gt;&amp;gt; &amp;lsquo;あ&amp;rsquo;[2] &amp;lsquo;\x82&amp;rsquo; &amp;gt;&amp;gt;&amp;gt; &amp;lsquo;あ&amp;rsquo;[3] Traceback (most recent call last): File &amp;ldquo;&amp;lt;stdin&amp;gt;&amp;rdquo;, line 1, in &amp;lt;module&amp;gt; IndexError: string index out of range この記事が参考になりました。qiita.com
これを知ったうえで愚直に書き換えると、
string=&amp;ldquo;パタトクカシーー&amp;rdquo; rev=&amp;ldquo;&amp;rdquo; for i in [1,3,5,7]: for j in range(3):#全角なので rev+=string[3*i+j] print rev</description></item></channel></rss>
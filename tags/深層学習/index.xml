<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>深層学習 on 1ミリもわからん</title><link>https://raahii.github.io/tags/%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92/</link><description>Recent content in 深層学習 on 1ミリもわからん</description><generator>Hugo -- gohugo.io</generator><language>ja-JP</language><lastBuildDate>Wed, 16 Oct 2019 21:58:50 +0900</lastBuildDate><atom:link href="https://raahii.github.io/tags/%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92/index.xml" rel="self" type="application/rss+xml"/><item><title>Ubuntu16.04でnvidiaドライバが再起動の度に無効になる</title><link>https://raahii.github.io/2019/10/16/nvidia-driver-not-work-after-reboot-on-ubuntu/</link><pubDate>Wed, 16 Oct 2019 21:58:50 +0900</pubDate><guid>https://raahii.github.io/2019/10/16/nvidia-driver-not-work-after-reboot-on-ubuntu/</guid><description>症状 Cudaのインストール手順を一通り済ませているにも関わらず，Ubuntuを起動するたびに nvidia-smi コマンドが実行できない．下記のようなエラーが吐かれる．
❯ nvidia-smi NVIDIA-SMI has failed because it couldn&amp;#39;t communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running. 解決策 原因は .run ファイルを使ってドライバのインストールをしていたからだった．下記のページが参考になった．
Nvidia driver not work after reboot on Ubuntu - NVIDIA Developer Forums とはいえ，Ubuntuの場合はパッケージマネージャからドライバを直接インストールできるので，aptを使ったほうが良いと思う．まずはppaを追加する．
❯ sudo add-apt-repository ppa:graphics-drivers/ppa ❯ sudo apt update 肝心のドライバのパッケージだが，検索すると色々出てくるのでインストールされているGPU及びCUDAに合ったバージョンを入れる．NvidiaのHPから検索ができる．
❯ sudo apt search &amp;#34;nvidia-[0-9]+\$&amp;#34; Sorting... Done Full Text Search... Done nvidia-304/xenial 304.137-0ubuntu0~gpu16.04.1 amd64 NVIDIA legacy binary driver - version 304.</description></item><item><title>Conditional Batch Normalizationについて</title><link>https://raahii.github.io/2018/12/12/conditional-batch-normalization/</link><pubDate>Wed, 12 Dec 2018 15:31:51 +0900</pubDate><guid>https://raahii.github.io/2018/12/12/conditional-batch-normalization/</guid><description>Batch Normalization Batch Normalization(BN)は，内部共変量シフトを軽減することで学習を効率化する手法である．特に学習の初期段階において，前段の層の出力分布が変化すると，後段の層はその変化自体に対応する必要がでてくるため，本質的な非線形関数の学習が阻害されてしまうという問題がある．この問題は層を増やせば増やすほど深刻となる．BNは各層の出力をミニバッチごとに正規化することにより分布の変化を抑制する．また重みの初期値への依存度を下げ，正則化を行う効果もある．
具体的には，入力バッチ $\mathcal{B}= {x_1,\cdot\cdot\cdot,x_m }$ に対して
$$\mu_{\mathcal{B}} \leftarrow \frac{1}{m} \sum_{i=0}^{m} x_i$$
$$\sigma^2_{\mathcal{B}} \leftarrow \frac{1}{m}\sum_{i=1}^{m}x_i$$
$$\hat{x}_i \leftarrow \frac{x_i - \mu_{\mathcal{B}}}{\sqrt{\sigma_{\mathcal{B}}^2+\epsilon}}$$
$$y_i \leftarrow \gamma\hat{x}_i + \beta$$
のように，標準化を施し，アフィン変換を行う（新たに平均$\beta$と分散$\gamma^2$を与えるとも言える?）．この$\beta$と$\gamma$がBNの学習パラメータである．また通常，上記の操作は入力特徴マップのチャネルごとに行う．よってパラメータ$\beta$と$\gamma$は長さチャネル数のベクトルとなる．
Conditional Batch Normalization Conditional Batch Normalization1(CBN)の”Conditional”の気持ちはクラスラベルをBNのパラメータ$\gamma$と$\beta$に組み込むところにある．どのように組み込むかというと，下図(右)のように両方のパラメータをクラスラベルを基にMLPでモデル化する（だけ）．
具体的には，入力データのラベルベクトル$c$があったとき，
$$ \Delta\mathcal{B} = MLP(c),\ \ \ \Delta \gamma = MLP(c) $$
のようにクラスラベルをBNのパラメータのチャネル数に合うようにMLPで変換し，
$$ \hat{\beta} = \beta + \Delta\mathcal{B},\ \ \ \hat{\gamma} = \gamma + \Delta\mathcal{\gamma},$$
のように新たなアフィン変換のパラメータとして用いる．引用したCBNの論文では自然言語のembeddingを用いているが，SNGAN2などではクラスラベルの1-of-Kベクタを用いているはず．
なにが嬉しいのか このあたりが自分もよく把握できていないのが正直なところ．CBN自体は先程触れたSNGANをきっかけに，SAGAN3，BigGAN4でも使われているが，その有無がどれほど精度に影響するのかはあまり言及されていない．おそらく直感的には，従来のような$G$および$D$の最初の層のみにクラスラベルを与えるよりも，様々なレベルの特徴マップに対してクラスラベルを活用するように仕向けることができるのだと思う．
また，各層にクラスラベルを組み込む方法を考えたとき，最もベーシックな方法は1-of-K表現のベクトルを特徴マップのサイズ（FHxFW）に拡大してチャネル方向に結合する手法だが，かなり冗長で，畳み込み演算との相性も微妙と思われる．そういう意味ではCBNを通してクラスラベルを組み込む方が理に適っている可能性はある．</description></item><item><title>3Dオブジェクト（ボクセルデータ）を描画するツールを作った</title><link>https://raahii.github.io/2017/10/09/voxel/</link><pubDate>Mon, 09 Oct 2017 01:32:00 +0900</pubDate><guid>https://raahii.github.io/2017/10/09/voxel/</guid><description>最近、GANで3Dオブジェクトを生成する論文を読んでいました。下のスライドは雑なまとめなのですが、前者が所謂3D-GANと呼ばれている論文で初めて3Dオブジェクトの生成にGANを適用した論文です。後者はその3D-GANを応用した研究のようです。
どんなものか知るには著者らが公開している動画が非常にわかりやすいです。静止画と同じようにGANを3Dモデルにも適用できそうだということがわかります。
ということでgithubにあがっている実装などを参考に実際にやろうとしているところなのですが、これらの手法では主に3Dオブジェクトを「ボクセル」として扱っています。
このボクセルというのは「体積 (volume)」と「ピクセル (pixel)」を組み合わせたかばん語らしいのですが、要は画像と同じ要領で3Dモデルを3次元配列に格納して表したものです。イメージとしてはマインクラフトみたいな感じです。
CGでは頂点情報や法線、テクスチャなどを保存する.obj, .3dsなどが有名（らしい）ですが、それらに比べると非常に簡単で取り扱いやすくなっています。
その一方でボクセルデータを保存する形式には.binvoxがあるのですが、メジャーではないためかツールが少なめです（まぁただの3次元配列だしね…）。ざっくり探したところ以下のものは便利そうだなと思いました。
.binvoxを3次元配列に変換: dimatura/binvox-rw-py(python)
.objや.mtlを読み込んで変換し、ボクセルとして可視化: Online Voxelizer(web)
ただ、単に.binvoxファイルをアップロードしてすぐに中身を見れるツールはなさそうだったので今回はそれを作ってみました（というエントリです）。ここから試せます。
github.com
ボクセルは形式そのものがシンプルなのでmatplotlibを使って3dplotするのでも良いのですが、結構重いんですよね。three.jsを使えばWeb上でマウスでグリグリできるインターフェースを簡単に作れるので楽しいですし、いつかフロントエンドで使えるかも…。
先程紹介したOnline Voxelizerに比べると動作がかなり遅いしメモリも結構消費してしまうのでそこが今後の課題です。うーん…おしまい。</description></item><item><title>人工知能とは</title><link>https://raahii.github.io/2015/11/14/174743/</link><pubDate>Sat, 14 Nov 2015 17:47:00 +0900</pubDate><guid>https://raahii.github.io/2015/11/14/174743/</guid><description>動機付け
最近、機械学習やディープラーンニングといった手法で人工知能研究が話題を集めている。特に今年学校で卒業研究が始まり、音声信号処理っぽいことをしているのだが、音声認識とか音声合成、声質変換あたりのこと調べると、すぐに統計的な手法に出会う。 例えば音声認識では、与えられた音声に対してそれが「あいうえお」のどの母音であるかをフォルマントという特徴量で判断したりするが、これを行うためには、あらかじめたくさんデータを用意して、第一フォルマントと第二フォルマントという特徴量の分布で、ここからここまでが「あ」ですよ、ここからは「い」ですよ、みたいな線引きをしてやらなければいけない。これはクラスタリング？という機械学習のアルゴリズムの一つだ。
はっきり言って、詳しいことは自分もわからないが、とにかくざっくりと「人工知能ってなんだ？」という質問にイメージをつけるべく、そのアウトラインをまとめようとこの文章を書いた。というのも、つい先日、「人工知能とか機械学習ってなんなの？」と聞かれ、「なんだろうね？笑」となってしまったのがきっかけである。
「人工知能は人間を超えるか」をざっくり　
人工知能は人間を超えるか ディープラーニングの先にあるもの (角川EPUB選書)
作者: 松尾豊 出版社/メーカー: KADOKAWA/中経出版 発売日: 2015/03/11 メディア: 単行本 この商品を含むブログ (8件) を見る 人工知能研究について 　そもそも、人工知能の定義とはなんだろう。これは専門家によってさまざまだが、一口に言えば「究極には人間と区別がつかない人工的な知能のこと」である。その人工知能を実現するために、さまざまな手法が考えられてきたわけだが、人工知能研究には現在までに3つのブームが存在したそうだ。これら3つこそが、人工知能をどのような手法で実現しようとしたのかを示す大きなまとまりになっている。これらを簡単にまとめることで、なんとなく機械学習やディープラーンニングが流行っている理由がわかるんじゃなかろうか。
第一のブーム 　1955~1970年頃のこと。その中身は主に推論と探索である。これは、いたってシンプルなルール によって現実の問題を解こうとするものである。
　ではいったいどんな問題を解いたのかというと、一つは迷路。これはスタート地点から考えられるすべての経路（場合分け）を順に調べることによってゴールを見つけるものである。主に探索木をモデルに用いていて、「深さ優先探索」や「幅優先探索」といったアルゴリズムが存在する。これらはわりと耳なじみがあると思う。他にも、ハノイの塔なども簡単に解ける。ハノイの塔にはちゃんとルールがあるので、それを満たすようにただシンプルな操作を繰り返すだけだ。
　一方、電脳戦で有名な将棋などのボードゲームに関しても、基本的に推論や探索で解く。しかしこの場合、「相手」がいることによって組み合わせが膨大になるため、すべての場合を列挙することは不可能だ。そのため、その時その時の盤面の状況に対して考えられる次の一手をすべて洗い出し、なんらかの評価手法でスコアをつけることによって最善に最善に手を打っていくことになる。これに関しては、盤面の評価に使用する特徴量(例えば飛車や角があるかないか、王がどこにいるか、など)が研究が進むにつれて良いものになっていったり、第3のブームで登場する機械学習を利用することによって現在も進化し続けているのだが、基本は推論、探索である。
　とにかく、単純な操作を多く繰り返す処理なら、パソコン用いた方が我々より早く正確であることは自明で、それを用いて賢く見せているにすぎない と言える(パソコン、インターネットが普及した今ではなんだか当たり前だけど)。お分かりのように、この推論や探索といった手法単体では、明確に定義されたルールの中で最適な解を求めることしかできず、いわゆるトイプロブレムは解けても、現実の複雑な問題を解くことはできない 。このような形で第一のブームは幕を閉じる。
第二のブーム 　1982~2000年くらいまで。その中身は機械に実践的な知識を持たせることである。第一のブームではシンプルな操作を繰り返すことがメインだったが、今度は必要な知識を機械に持たせることによってエキスパートシステムを作ろうと試みたのである。
　実際に開発されたエキスパートシステムには、1970年にスタンフォード大学でつくられたMYCINなどが挙げられる。MYCINは伝染病の血液疾患の患者を診断し、抗生物質を処方する、いわば専門医の代わりとなるシステムである。あらかじめ用意した500のルールに従って患者に質問を行い、条件分岐して患者が感染した細菌を特定し、適切な抗生物質を処方する。他にも、比較的最近話題となった、自分が思い浮かべたアニメキャラや有名人などを言い当てる「アキネイター」もこの種のシステムだろう。
　このようなエキスパートシステムを作るのには、知識を集めるためのコストがかかること、そして、知識が増えれば増えるほど、条件分岐が複雑かつ膨大になり、矛盾も発生するといった問題がついて回る。しかし、コンピュータの性能が日進月歩である今、それらは大した問題ではない。それよりも、次に上げる二つの致命的な問題がある。
　一つは、人間にとって常識レベルの知識が途方もなく膨大であるということである。例えば、機械翻訳をする時、&#34;He saw a woman in the garden with a telescope.&#34;という文章を日本語に訳そうとすると、in the gardenとwith a telescopeの節がHeにかかるのかwomanにかかるのか一意に定まらない。しかし、人間であれば常識的に「彼は望遠鏡で庭にいる女性を見た。」と訳すことができる。このように、エキスパートシステムを拡張させ、より柔軟なものにしようとした時、そこには「常識的な判断」というものが必要不可欠になるが、それが非常に難しかったのである。</description></item></channel></rss>
<!doctype html><html lang=ja-jp><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=generator content="Hugo 0.61.0"><title>AboutMe - 1ミリもわからん</title><link rel=canonical href=https://raahii.github.io/about/><link href=https://raahii.github.io/favicon.ico rel=icon type=image/x-icon><meta property="og:title" content="AboutMe"><meta property="og:description" content="About Me   Name: Yuki Nakahira
  Master's student, Science and Engineering at University of Chiba
  Mail: piyo56.net@gmail.com
  Links:
 Github Twitter Labo    Interest:
 Web backend (golang) Computer Vision (GAN) Football ⚽ Fashion 👖    Recent coding status:
  Work App, Web service ・ShowTime (2019 / 08, M2)  &ldquo;あなたのプレゼンをもっと面白く&rdquo;
  ShowTimeはプレゼンの際に発表者のポーズに応じてスライドを進めたり，効果音を鳴らすことで，発表にメリハリと笑いを与えてくれるツールです． HackU2019にて制作．発表者のポーズ認識（機械学習）の実装，及びサーバーサイドを担当．
  Links: Github, Hack U 2019 TOKYO Student Hackathon - Yahoo!"><meta property="og:type" content="article"><meta property="og:url" content="https://raahii.github.io/about/"><meta name=twitter:card content="summary"><meta name=twitter:title content="AboutMe"><meta name=twitter:description content="About Me   Name: Yuki Nakahira
  Master's student, Science and Engineering at University of Chiba
  Mail: piyo56.net@gmail.com
  Links:
 Github Twitter Labo    Interest:
 Web backend (golang) Computer Vision (GAN) Football ⚽ Fashion 👖    Recent coding status:
  Work App, Web service ・ShowTime (2019 / 08, M2)  &ldquo;あなたのプレゼンをもっと面白く&rdquo;
  ShowTimeはプレゼンの際に発表者のポーズに応じてスライドを進めたり，効果音を鳴らすことで，発表にメリハリと笑いを与えてくれるツールです． HackU2019にて制作．発表者のポーズ認識（機械学習）の実装，及びサーバーサイドを担当．
  Links: Github, Hack U 2019 TOKYO Student Hackathon - Yahoo!"><link rel=icon href=https://raahii.github.io/images/favicon.ico type=image/x-icon><link rel=stylesheet href=https://raahii.github.io/css/main.css media=all><link rel=stylesheet href="//fonts.googleapis.com/css?family=Merriweather:400|Lato:400,400italic,700"></head><body><div class=wrapper><header class=header><nav class=nav><a href=https://raahii.github.io/ class=nav-logo><img src=https://raahii.github.io/images/bird.svg width=45 height=45 alt=Logo><p class=nav-title>1ミリもわからん</p></a><ul class=nav-links><li><a href=/tags/>Tags</a></li><li><a href=/about/>About</a></li><li><a href=https://github.com/raahii target=_blank>GitHub</a></li><li><a href=https://twitter.com/raahiiy target=_blank>Twitter</a></li></ul></nav></header><main class=content role=main><article class=article><div class=article-content><h1 id=about-me>About Me</h1><ul><li><p>Name:
Yuki Nakahira</p></li><li><p>Master's student, <a href=http://www.se.chiba-u.jp/>Science and Engineering</a> at <a href=http://www.chiba-u.ac.jp/>University of Chiba</a></p></li><li><p>Mail:
<a href=mailto:piyo56.net@gmail.com>piyo56.net@gmail.com</a></p></li><li><p>Links:</p><ul><li><a href=https://github.com/raahii>Github</a></li><li><a href=https://twitter.com/raahiiy>Twitter</a></li><li><a href=https://www.kawa-lab.org/>Labo</a></li></ul></li><li><p>Interest:</p><ul><li>Web backend (<a href=https://gopherize.me/>golang</a>)</li><li>Computer Vision (<a href=https://en.wikipedia.org/wiki/Generative_adversarial_network>GAN</a>)</li><li>Football ⚽</li><li>Fashion 👖</li></ul></li><li><p>Recent coding status:</p></li></ul><h2 id=work>Work</h2><h3 id=app-web-service>App, Web service</h3><h4 id=showtime-2019--08-m2>・ShowTime (2019 / 08, M2)</h4><blockquote><p>&ldquo;あなたのプレゼンをもっと面白く&rdquo;</p></blockquote><blockquote><p>ShowTimeはプレゼンの際に発表者のポーズに応じてスライドを進めたり，効果音を鳴らすことで，発表にメリハリと笑いを与えてくれるツールです．
HackU2019にて制作．発表者のポーズ認識（機械学習）の実装，及びサーバーサイドを担当．</p></blockquote><blockquote><p>Links: <a href=https://github.com/pocket-dan/showtime>Github</a>, <a href=https://hacku.yahoo.co.jp/hacku2019tokyo/>Hack U 2019 TOKYO Student Hackathon - Yahoo! JAPAN</a></p></blockquote><h4 id=-2018--10-m1>・ばえるーポン (2018 / 10, M1)</h4><blockquote><p>SNSにアップされた写真のインスタ映え度を深層学習で判定し，そのスコアに応じて割引クーポンなどの報酬を付与するサービス．<a href=https://2018.jphacks.com/>JPHACKS2018</a>で開発．東京会場優勝．全国147チーム中2位（相当）．サーバーサイド全般と深層学習を用いてインスタ映え度推定のための食べ物検出を担当（動画40秒あたり）．</p></blockquote><blockquote><p>Links: <a href=https://github.com/jphacks/TK_1810>Github</a>, <a href="https://2018.jphacks.com/result/?fbclid=IwAR06esEhUEgm7AKqRloqswW_1VERsoxxkFW6ykdFHdW5G4sH40H8PT8eNyE">結果報告 | JPHACKS 2018</a></p></blockquote><h4 id=arxiv-equationshttpsarxiv-equationsnetlifycom--2019--02-m1->・<a href=https://arxiv-equations.netlify.com>Arxiv Equations</a> ( 2019 / 02, M1 )</h4><figure><img src=https://user-images.githubusercontent.com/13511520/52259980-d2695700-2967-11e9-841d-6b1f4822755e.gif width=90%></figure><blockquote><p>最近，一部の研究分野では<a href=https://arxiv.org/>arxiv</a>と呼ばれる論文のプレプリントを保存・公開するサービスが広く使われている．本サービスはarxivの論文から数式を自動で抽出し，latex形式でコピーできるというもの．</p></blockquote><blockquote><p>Links: <a href=https://github.com/raahii/arxiv-equations>Github</a></p></blockquote><h3 id=deep-learning>Deep Learning</h3><h4 id=dcvgan-depth-conditional-video-generation--2019--04-m2->・DCVGAN: Depth Conditional Video Generation ( 2019 / 04, M2 )</h4><figure><img src=https://user-images.githubusercontent.com/13511520/57762583-baf5f300-773a-11e9-942d-858c2d834536.png width=60%></figure><blockquote><p><a href=http://2019.ieeeicip.org/>ICIP2019</a>採択論文の公式実装．シーンの幾何学的な情報であるデプスを用いてカラー動画生成を行う手法提案．
keyword: Generative Adversarial Networks (GAN), domain translation, video generation</p></blockquote><blockquote><p>Links: <a href=https://github.com/raahii/dcvgan>Github</a></p></blockquote><h4 id=video-gans-evaluation--2019--01-m1->・video-gans-evaluation ( 2019 / 01, M1 )</h4><figure><img src=https://user-images.githubusercontent.com/13511520/51083747-45652080-1762-11e9-880f-88139b9cc66d.png width=100%></figure><blockquote><p>動画生成系のGANを評価するためのフレームワークを提案，有名なメトリックを3つサポート．</p></blockquote><blockquote><p>Links: <a href=https://github.com/raahii/video-gans-evaluation>Github</a></p></blockquote><h4 id=mocogan-chainer--2017--12-b4>・mocogan-chainer ( 2017 / 12, B4)</h4><figure><img src=https://github.com/raahii/mocogan-chainer/raw/master/doc/mug/normal.gif width=50%></figure><blockquote><p>動画を生成する「MoCoGAN」と呼ばれる深層学習モデルのChainerによる再現実装．</p></blockquote><blockquote><p>Links: <a href=https://github.com/raahii/mocogan-chainer>Github</a></p></blockquote><h4 id=3dgan-chainer--2017--10-b4>・3dgan-chainer ( 2017 / 10, B4)</h4><figure><img src=/images/aboutme/3dgan-chainer.png width=70%></figure><blockquote><p>3Dのボクセルデータを生成する「3DGAN」と呼ばれる深層学習モデルのChainerによる再現実装．</p></blockquote><blockquote><p>Links: <a href=https://github.com/raahii/3dgan-chainer>Github</a></p></blockquote><h2 id=internship>Internship</h2><ul><li>2016 / 08 : <a href=https://www.atware.co.jp/>株式会社アットウェア</a> (Java, SpringBoot, Tomcat)</li><li>2016 / 10 - 2018 / 02: <a href=https://www.traicy.com/>株式会社トライシージャパン</a> (PHP, Ruby)</li><li>2017 / 02 : <a href=https://pepabo.com/>GMOペパボ株式会社</a> (Ruby, Rails)</li><li>2017 / 08 : <a href=https://info.cookpad.com/>クックパッド株式会社</a> (Ruby, Rails, Java)</li><li>2018 / 04 - 2018 / 07: <a href=https://www.pieceofcake.co.jp/>株式会社ピースオブケイク</a> (Node, Vue.js)</li><li>2019 / 02 : <a href=https://www.cyberagent.co.jp/corporate/overview/>株式会社サイバーエージェント</a> (Go, AWS)</li><li>2019 / 04 - : <a href=https://www.fixstars.com/ja/>株式会社フィックスターズ</a> (Go, Python, Nuxt.js, AWS)</li></ul><h2 id=paper--conference>Paper / Conference</h2><ul><li><p><a href="http://cvim.ipsj.or.jp/index.php?id=cvim212">CVIM</a> / Poster / Osaka (2018 / 05) | <a href=https://www.kawa-lab.org/archives/726>labo</a></p><p>中平有樹, 川本一彦, RGB-D動画生成のためのGAN，情報処理学会研究報告. Vol. 2018-CVIM-212, No.28, pp.1-7, 2018.</p></li><li><p><a href="http://cvim.ipsj.or.jp/index.php?id=cvim213">CVIM</a> / Oral&Poster / Fukuoka (2018 / 09) | <a href=https://www.kawa-lab.org/archives/769>labo</a></p><p>中平有樹, 川本一彦, デプスからカラーへのドメイン変換を用いたGANによる動画生成，情報処理学会研究報告. Vol. 2018-CVIM-213, No.32, pp.1-7, 2018.</p></li><li><p><a href=https://apsipa2018.org/Default.asp>APSIPA2018</a> / Oral / Hawaii (2018 / 11) | <a href=https://www.kawa-lab.org/archives/786>labo</a></p><p>Yuki Nakahira and Kazuhiko Kawamoto, Generative adversarial networks for generating RGB-D videos, Proc. of Asia-Pacific Signal and Information Processing, Association Annual Summit and Conference, pp.1276-1281, 2018. /</p></li><li><p><a href=http://2019.ieeeicip.org/>ICIP2019</a> / Oral / Taiwan (2019 / 09) | <a href=https://www.kawa-lab.org/archives/1055>labo</a>, <a href=https://github.com/raahii/dcvgan>github</a></p><p>Yuki Nakahira and Kazuhiko Kawamoto, <a href=https://github.com/raahii/dcvgan>DCVGAN: Depth Conditional Video Generation</a>, 2019 IEEE International Conference on Image Processing, ICIP 2019.</p></li></ul><h2 id=awards>Awards</h2><ul><li>CVIM2018年9月研究会 <a href="http://cvim.ipsj.or.jp/index.php?id=award_winner#2018">奨励賞</a> (2018 / 09)</li><li>JPHACKS2018 東京 <a href=https://jphacks.com/information/jphacks-award-finalists/>優勝</a> (2018 / 10)</li><li>JPHACKS2018 全国 <a href=https://jphacks.com/information/result-report/>Best Audience Award</a> (2018 / 11)</li><li>HackU2019 東京 <a href=https://hacku.yahoo.co.jp/hacku2019tokyo/>最優秀賞</a> (2019 / 08)</li></ul></div></article><div id=disqus_thread></div><script type=application/javascript>var disqus_config=function(){};(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById('disqus_thread').innerHTML='Disqus comments not available by default when the website is previewed locally.';return;}
var d=document,s=d.createElement('script');s.async=true;s.src='//'+"raahii"+'.disqus.com/embed.js';s.setAttribute('data-timestamp',+new Date());(d.head||d.body).appendChild(s);})();</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></main><footer class=footer><ul class=footer-links><li><a href=https://gohugo.io/ class=footer-links-kudos>Made with <img src=https://raahii.github.io/images/hugo-logo.png width=22 height=22 alt="hugo log"></a></li></ul></footer></div><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');ga('create','UA-86522604-4','auto');ga('send','pageview');}</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.10.1/dist/katex.min.css integrity=sha384-dbVIfZGuN1Yq7/1Ocstc1lUEm+AT+/rCkibIcC/OmWo5f0EA48Vf8CytHzGrSwbQ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.10.1/dist/katex.min.js integrity=sha384-2BKqo+exmr9su6dir+qCw08N2ZKRucY4PrGQPPWU1A7FtlCGjmEGFqXCv5nyM5Ij crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.10.1/dist/contrib/auto-render.min.js integrity=sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI crossorigin=anonymous onload=renderMathInElement(document.body);></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:true},{left:"$",right:"$",display:false}]});});</script></body></html>